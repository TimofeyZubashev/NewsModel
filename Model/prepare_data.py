import string
import re

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
from nltk import ngrams

from pymorphy2 import MorphAnalyzer

N = 2
text = (
'''
üëßüë¶ –í –∑–∞–≥–æ—Ä–æ–¥–Ω—ã—Ö –¥–µ—Ç—Å–∫–∏—Ö –ª–∞–≥–µ—Ä—è—Ö –ú–∞—Ä–∏–π –≠–ª –≤ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –æ—Ç–¥—ã—Ö–∞—é—Ç 2335 –¥–µ—Ç–µ–π, –∏ –µ—â—ë 12265 —Ä–µ–±—è—Ç –ø–æ—Å–µ—â–∞—é—Ç 175 –ø—Ä–∏—à–∫–æ–ª—å–Ω—ã—Ö –ª–∞–≥–µ—Ä–µ–π.

–í—Å–µ–≥–æ –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ —Ä–µ—Å–ø—É–±–ª–∏–∫–∏ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç—å 200 –æ–∑–¥–æ—Ä–æ–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –¥–µ—Ç–µ–π –∏ –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –≤ –ø–µ—Ä–≤—É—é —Å–º–µ–Ω—É –±—É–¥—É—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞—Ç—å 192 –ª–∞–≥–µ—Ä—è, –≤–∫–ª—é—á–∞—è 13 –∑–∞–≥–æ—Ä–æ–¥–Ω—ã—Ö, 177 –ø—Ä–∏—à–∫–æ–ª—å–Ω—ã—Ö –∏ 2 –ø–∞–ª–∞—Ç–æ—á–Ω—ã—Ö.

–í—Å–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—á–Ω—É—Ç —Ä–∞–±–æ—Ç—É –≤ –ø–µ—Ä–≤—É—é —Å–º–µ–Ω—É, –∏–º–µ—é—Ç —Å–∞–Ω–∏—Ç–∞—Ä–Ω–æ-—ç–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –æ—Ç –†–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä–∞. –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ª–∞–≥–µ—Ä—è—Ö, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã—Ö –Ω–∞ –±–µ—Ä–µ–≥—É –æ–∑–µ—Ä, –¥–µ—Ç–∏ —Å–º–æ–≥—É—Ç –∫—É–ø–∞—Ç—å—Å—è, –≤–µ–¥—å –∏—Ö –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª—É—á–∏–ª–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –æ—Ç –Ω–∞–¥–∑–æ—Ä–Ω–æ–π –∏–Ω—Å—Ç–∞–Ω—Ü–∏–∏.
'''
)

nltk.download('punkt')
STOPWORDS_AND_CHARS = stopwords.words('russian')
emoji_finder = re.compile('[\U0001F300-\U0001F64F\U0001F680-\U0001F6FF\u2600-\u26FF\u2700-\u27BF]+')

COUNT_RUSSIAN_LETTERS = 33
RUSSIAN_ALPHABET = [chr(0x0410 + index) for index in range(COUNT_RUSSIAN_LETTERS)]
RUSSIAN_ALPHABET.extend([chr(0x0430 + index) for index in range(COUNT_RUSSIAN_LETTERS)])
STOPWORDS_AND_CHARS.extend(string.punctuation)
STOPWORDS_AND_CHARS.extend(RUSSIAN_ALPHABET)

def remove_digits(text: str) -> str:
    return [word for word in text if (word not in string.digits) and (word not in string.punctuation)]

def remove_stop_words(text: str, stopwords=STOPWORDS_AND_CHARS) -> list:
    return [word for word in text.split(' ') if word not in stopwords]

def lemmatize(text) -> str:
    pymorphy2_analyzer = MorphAnalyzer()
    return ' '.join([pymorphy2_analyzer.parse(word)[0].normal_form.strip() for word in text.split(' ')]).strip()

def tokenize(text) -> list:
    tokenized = sent_tokenize(text)
    tokens = []
    for token in tokenized:
        with_no_tabs = token.replace('\n', '').replace('\xa0', '')
        with_no_emoji = re.sub(emoji_finder, '', with_no_tabs)
        tokens.append(with_no_emoji.strip())

    return tokens


def to_bigrams(tokens: list):
    return ngrams(''.join(tokens).split(), N)


after_remove_dg = remove_digits(text)
print(f'–ø–æ—Å–ª–µ —Ü–∏—Ñ—Ä: {after_remove_dg}')
after_remove_stop_words = remove_stop_words(''.join(after_remove_dg))
print(f'–ø–æ—Å–ª–µ —Å—Ç–æ–ø —Å–ª–æ–≤: {after_remove_stop_words}')
after_lemmatize = lemmatize(' '.join(after_remove_stop_words))
print(f'–ø–æ—Å–ª–µ –º–æ—Ä—Ñ—ã: {after_lemmatize}')
after_tokenize = tokenize(after_lemmatize)
print(f'—Ç–æ–∫–µ–Ω—ã: {after_tokenize}')
print(*to_bigrams(after_tokenize))
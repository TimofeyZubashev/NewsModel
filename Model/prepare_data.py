import string
import re
from dataclasses import dataclass

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
from nltk import ngrams

from pymorphy2 import MorphAnalyzer


nltk.download('stopwords')
nltk.download('punkt')
STOPWORDS_AND_CHARS = stopwords.words('russian')
emoji_finder = re.compile('[\U0001F300-\U0001F64F\U0001F680-\U0001F6FF\u2600-\u26FF\u2700-\u27BF]+')

COUNT_RUSSIAN_LETTERS = 33
RUSSIAN_ALPHABET = [chr(0x0410 + index) for index in range(COUNT_RUSSIAN_LETTERS)]
RUSSIAN_ALPHABET.extend([chr(0x0430 + index) for index in range(COUNT_RUSSIAN_LETTERS)])
STOPWORDS_AND_CHARS.extend(string.punctuation)
STOPWORDS_AND_CHARS.extend(RUSSIAN_ALPHABET)

#def to_bigrams(tokens: list):
#    return ngrams(''.join(tokens).split(), N)


@dataclass
class TextPreprocess:
    ''' –≤—Å–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π '''
    stopwords = STOPWORDS_AND_CHARS
    post: str # –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ —Å–ø–∏—Å–∫–æ—Å, —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ,
              # –∫–æ—Ç–æ—Ä–æ–µ —Ö—Ä–∞–Ω–∏—Ç –≤ —Å–µ–±–µ –≤—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π

    def clear_all(self):
        # —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        self.remove_shit()
        # –ø—Ä–∏–≤–µ–¥–µ–∏–µ –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É
        self.lemmatize()
        # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        self.tokenize()

    def remove_shit(self) -> None:
        # —É–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏ —Å–∏–º–≤–æ–ª–æ–≤ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫–∏ –∏ —Ç–∏—Ä–µ
        without_emojis_n_tabs = re.sub(emoji_finder, '', self.post).replace('\n', '').replace('\xa0', '')
        without_links = re.sub(r'^https?:\/\/.*[\r\n]*', '', without_emojis_n_tabs, flags=re.MULTILINE)
        # —É–¥–∞–ª–µ–Ω–∏–µ —Ü–∏—Ñ—Ä –∏ —Å—Ç–æ–ø —Å–ª–æ–≤
        without_digits = (
            ''.join([word for word in without_links if (word not in string.punctuation) and (word not in string.digits)])
        )
        self.post = ' '.join([word for word in without_digits.split(' ') if word not in self.stopwords])

    def lemmatize(self) -> None:
        pymorphy2_analyzer = MorphAnalyzer()
        self.post = ' '.join([pymorphy2_analyzer.parse(word)[0].normal_form.strip() for word in self.post.split(' ')]).strip()

    def tokenize(self) -> None:
        self.post = sent_tokenize(' '.join([word.strip() for word in self.post.split(' ')]))


if __name__ == '__main__':
    # —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Å—Ç–∞, –æ–±—ã—á–Ω–æ —ç—Ç–æ —É–ª—Å–æ–≤–∏–µ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è

    processor = TextPreprocess(post='''
    üëßüë¶ –í –∑–∞–≥–æ—Ä–æ–¥–Ω—ã—Ö –¥–µ—Ç—Å–∫–∏—Ö –ª–∞–≥–µ—Ä—è—Ö –ú–∞—Ä–∏–π –≠–ª –≤ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –æ—Ç–¥—ã—Ö–∞—é—Ç 2335 –¥–µ—Ç–µ–π, –∏ –µ—â—ë 12265 —Ä–µ–±—è—Ç –ø–æ—Å–µ—â–∞—é—Ç 175 –ø—Ä–∏—à–∫–æ–ª—å–Ω—ã—Ö –ª–∞–≥–µ—Ä–µ–π.

    –í—Å–µ–≥–æ –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ —Ä–µ—Å–ø—É–±–ª–∏–∫–∏ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç—å 200 –æ–∑–¥–æ—Ä–æ–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –¥–µ—Ç–µ–π –∏ –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –≤ –ø–µ—Ä–≤—É—é —Å–º–µ–Ω—É –±—É–¥—É—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞—Ç—å 192 –ª–∞–≥–µ—Ä—è, –≤–∫–ª—é—á–∞—è 13 –∑–∞–≥–æ—Ä–æ–¥–Ω—ã—Ö, 177 –ø—Ä–∏—à–∫–æ–ª—å–Ω—ã—Ö –∏ 2 –ø–∞–ª–∞—Ç–æ—á–Ω—ã—Ö.

    –í—Å–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—á–Ω—É—Ç —Ä–∞–±–æ—Ç—É –≤ –ø–µ—Ä–≤—É—é —Å–º–µ–Ω—É, –∏–º–µ—é—Ç —Å–∞–Ω–∏—Ç–∞—Ä–Ω–æ-—ç–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –æ—Ç –†–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä–∞. –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ª–∞–≥–µ—Ä—è—Ö, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã—Ö –Ω–∞ –±–µ—Ä–µ–≥—É –æ–∑–µ—Ä, –¥–µ—Ç–∏ —Å–º–æ–≥—É—Ç –∫—É–ø–∞—Ç—å—Å—è, –≤–µ–¥—å –∏—Ö –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª—É—á–∏–ª–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –æ—Ç –Ω–∞–¥–∑–æ—Ä–Ω–æ–π –∏–Ω—Å—Ç–∞–Ω—Ü–∏–∏.
    ''')
    processor.clear_all()
    print(processor.post)
